{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задание 1\n",
    "Сгенерировать последовательности, которые бы состояли из цифр (от 0 до 9)\n",
    "и задавались следующим образом:<br>\n",
    "x - последовательность цифр<br>\n",
    "y1 = x1, y(i) = x(i) + x(1). Если y(i) >= 10, то y(i) = y(i) - 10<br>\n",
    "\n",
    "Задача:<br>\n",
    "научить модель предсказывать y(i) по x(i)<br>\n",
    "пробовать RNN, LSTM, GRU<br>\n",
    "\n",
    "#### Задание 2 \n",
    "(дополнительное и необязательное)\n",
    "применить LSTM для решения лекционного практического задания"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# импорт библиотек\n",
    "import torch\n",
    "from torch import nn\n",
    "import random\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задание 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Подготовка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.1 Генерация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([tensor([4, 5, 2, 0, 5, 1, 5, 0, 6, 5, 6, 6, 2, 5, 8, 0, 8, 4, 5, 6, 3, 5, 6, 4,\n",
       "          1]),\n",
       "  tensor([0, 4, 0, 7, 8, 1, 1, 0, 4, 1, 3, 8, 0, 2, 2, 4, 5, 8, 0, 3, 5, 6, 7, 6,\n",
       "          7, 5, 6, 8, 1, 2, 5, 5, 7, 4, 3, 2, 3, 4, 6, 3, 3, 7, 3, 1, 3, 1, 6, 7,\n",
       "          7, 6, 7, 6, 1, 7, 6, 7, 2, 7, 7, 8, 7, 1, 3, 0, 8, 1, 5, 1, 0, 2, 7, 2,\n",
       "          0, 3, 4]),\n",
       "  tensor([1, 6, 4, 3, 5, 7, 1, 4, 3, 5, 7, 0, 4, 4, 1, 2, 0, 5, 0, 8, 4, 7, 1, 0,\n",
       "          1, 5, 0, 3, 4, 3, 6, 6, 3, 6, 3, 4, 6, 2, 0, 1, 4, 8, 3, 4, 1, 2, 1, 6,\n",
       "          5, 8, 1, 3, 8, 8, 3, 6, 4, 1, 0, 6, 7, 7, 0, 6, 0, 6, 0, 2, 2, 2, 5, 0,\n",
       "          6, 8, 2, 4, 5, 4, 7, 1, 3, 1, 6, 4, 4, 6, 2, 7, 2, 3, 4, 2, 2, 6, 1, 6,\n",
       "          1, 1, 5, 2, 6, 8, 4, 7, 6, 6, 0, 4, 6, 2, 5, 0, 8, 5, 0, 3, 2, 5, 0, 6,\n",
       "          8, 6, 0, 5, 4, 4, 8, 2, 3, 4, 4, 6, 4, 8, 7, 7, 2, 2, 8, 7, 1, 7, 2, 8,\n",
       "          8, 8, 2, 3, 8, 5])],\n",
       " [tensor([4, 9, 6, 4, 9, 5, 9, 4, 0, 9, 0, 0, 6, 9, 2, 4, 2, 8, 9, 0, 7, 9, 0, 8,\n",
       "          5]),\n",
       "  tensor([0, 4, 0, 7, 8, 1, 1, 0, 4, 1, 3, 8, 0, 2, 2, 4, 5, 8, 0, 3, 5, 6, 7, 6,\n",
       "          7, 5, 6, 8, 1, 2, 5, 5, 7, 4, 3, 2, 3, 4, 6, 3, 3, 7, 3, 1, 3, 1, 6, 7,\n",
       "          7, 6, 7, 6, 1, 7, 6, 7, 2, 7, 7, 8, 7, 1, 3, 0, 8, 1, 5, 1, 0, 2, 7, 2,\n",
       "          0, 3, 4]),\n",
       "  tensor([1, 7, 5, 4, 6, 8, 2, 5, 4, 6, 8, 1, 5, 5, 2, 3, 1, 6, 1, 9, 5, 8, 2, 1,\n",
       "          2, 6, 1, 4, 5, 4, 7, 7, 4, 7, 4, 5, 7, 3, 1, 2, 5, 9, 4, 5, 2, 3, 2, 7,\n",
       "          6, 9, 2, 4, 9, 9, 4, 7, 5, 2, 1, 7, 8, 8, 1, 7, 1, 7, 1, 3, 3, 3, 6, 1,\n",
       "          7, 9, 3, 5, 6, 5, 8, 2, 4, 2, 7, 5, 5, 7, 3, 8, 3, 4, 5, 3, 3, 7, 2, 7,\n",
       "          2, 2, 6, 3, 7, 9, 5, 8, 7, 7, 1, 5, 7, 3, 6, 1, 9, 6, 1, 4, 3, 6, 1, 7,\n",
       "          9, 7, 1, 6, 5, 5, 9, 3, 4, 5, 5, 7, 5, 9, 8, 8, 3, 3, 9, 8, 2, 8, 3, 9,\n",
       "          9, 9, 3, 4, 9, 6])])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# функция для генерации данных\n",
    "def gen_data(data_len): #принимает на вход длину данных\n",
    "    X = torch.randint(0, 9, (data_len,)) # создаёт тензор из случайного набора цифр от 0 до 9 заданной длины\n",
    "    Y = torch.zeros(data_len).type(torch.int64) # создаёт тензор заданной длины с нулями\n",
    "    x1 = X[0]\n",
    "    Y[0] = x1\n",
    "    for i in range(1, len(X)): # заполняет тензо Y данными\n",
    "        Y[i] = X[i] + x1\n",
    "        if Y[i] >= 10:\n",
    "            Y[i] -= 10\n",
    "    return X,Y\n",
    "X1,Y1 = gen_data(25)\n",
    "X2,Y2 = gen_data(75)\n",
    "X3,Y3 = gen_data(150)\n",
    "X = [X1, X2, X3]\n",
    "Y = [Y1, Y2, Y3]\n",
    "X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# настройки\n",
    "epoch = 1000          # количество эпох\n",
    "dict_size = 10        # длина \"словаря\", в нашем случае 10 символов\n",
    "batch_size = 5      # размер батча\n",
    "embedding_size = 15 # размер эмбеддинга\n",
    "hidden_size = 10    # размер скрытого состояния"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция обучения и проверки модели\n",
    "def test_model(model, optimizer, loss_data, accuracy_data, epochs=epoch):\n",
    "    for i in range(3): # проходит по трём вариантам длины последовательности\n",
    "        x = X[i]\n",
    "        y = Y[i]\n",
    "        print(\"Data length: {}\".format(len(x)))\n",
    "        for ep in range(epochs):\n",
    "            train_loss = 0.\n",
    "            train_passed = 0\n",
    "            for i in range(int(len(x) / batch_size)):\n",
    "                X_batch = x[i * batch_size: (i + 1) * batch_size]\n",
    "                Y_batch = y[i * batch_size: (i + 1) * batch_size]\n",
    "                optimizer.zero_grad()\n",
    "                answers = model.forward(X_batch)\n",
    "                loss = criterion(answers, Y_batch)\n",
    "                train_loss += loss.item()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                train_passed += 1\n",
    "            if (ep+1) % 250 == 0:\n",
    "                print(\"Epoch {}, Train loss: {:.3f}\".format(ep+1, train_loss / train_passed))\n",
    "        loss_data.append(train_loss / train_passed)\n",
    "        accuracy = float((model(x).argmax(axis=1) == y).sum() / len(y))\n",
    "        accuracy_data.append(accuracy)\n",
    "        print(\"Accuracy: {:.3f}\".format(accuracy))\n",
    "        optimizer.zero_grad()\n",
    "        for layer in model.children():\n",
    "            if hasattr(layer, 'reset_parameters'):\n",
    "                layer.reset_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.2 Создание класса сети"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# создаём класс для сети\n",
    "class NeuralNetwork(nn.Module):\n",
    "    # параметры сети\n",
    "    def __init__(self, \n",
    "                 rnnClass,                           # класс RNN, можем передавать любой, от этого будет зависеть, какая RNN ячейка будет создана\n",
    "                 dict_size = dict_size,              # размер словаря\n",
    "                 embedding_size = embedding_size,    # размер эмбеддинга\n",
    "                 hidden_size = hidden_size           # размер скрытого состояния\n",
    "                ):\n",
    "        super().__init__()\n",
    "        \n",
    "        # в модели будем сохранять\n",
    "        self.num_hiddens = hidden_size # размерность скрытого состояния\n",
    "        self.embedding = nn.Embedding(dict_size, embedding_size) # слой эмбеддингов (на вход принимает размерность словаря и размерность эмбеддингов)\n",
    "        # формируем два линейных слоя\n",
    "        self.hidden = rnnClass(embedding_size, hidden_size, batch_first=True) # создаём RNN ячейку переданного в модель класса, на вход принимает\n",
    "                                                                              # размер эмбеддинга, размер скрытого состояния и настройка, на каком месте в передаваемой \n",
    "                                                                              # последовательности стоит батч - на первом или на втором\n",
    "        self.output = nn.Linear(hidden_size, dict_size) # последний линейный слой, который переводит размерность скрытого состояния в размерность словаря\n",
    "        \n",
    "    def forward(self, X): # функция forward принимает на вход батч элементов\n",
    "        out = self.embedding(X) # прогоняем их через эмбеддинг, который возвращает преобразованные данные\n",
    "        outputs, state = self.hidden(out) # отправляем эти преобразованные данные в слой скрытого состояния, на выход получаем outputs и state модели после\n",
    "                                    # последнего элемента; в данном случае можем использовать state, т.к. предсказываем значение последнего элемента\n",
    "                                    # outputs - скрытые состояния для каждого из 40 элементов\n",
    "                                    # state - состояние модели после последнего элемента, которое показывает уверенность модели в каждом элементе словаря\n",
    "        predictions = self.output(outputs)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# задаём функцию потерь и оптимизатор\n",
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Обучение сетей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.1 RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data length: 25\n",
      "Epoch 250, Train loss: 1.472\n",
      "Epoch 500, Train loss: 1.066\n",
      "Epoch 750, Train loss: 0.819\n",
      "Epoch 1000, Train loss: 0.652\n",
      "Accuracy: 0.920\n",
      "Data length: 75\n",
      "Epoch 250, Train loss: 0.815\n",
      "Epoch 500, Train loss: 0.392\n",
      "Epoch 750, Train loss: 0.226\n",
      "Epoch 1000, Train loss: 0.149\n",
      "Accuracy: 1.000\n",
      "Data length: 150\n",
      "Epoch 250, Train loss: 0.434\n",
      "Epoch 500, Train loss: 0.177\n",
      "Epoch 750, Train loss: 0.110\n",
      "Epoch 1000, Train loss: 0.082\n",
      "Accuracy: 0.993\n"
     ]
    }
   ],
   "source": [
    "RNN = NeuralNetwork(torch.nn.RNN)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(RNN.parameters(), lr=.001)\n",
    "loss_RNN = []\n",
    "accuracy_RNN = []\n",
    "test_model(RNN, optimizer, loss_RNN, accuracy_RNN)\n",
    "# loss_RNN, accuracy_RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2 LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data length: 25\n",
      "Epoch 250, Train loss: 2.143\n",
      "Epoch 500, Train loss: 2.004\n",
      "Epoch 750, Train loss: 1.870\n",
      "Epoch 1000, Train loss: 1.730\n",
      "Accuracy: 0.480\n",
      "Data length: 75\n",
      "Epoch 250, Train loss: 2.121\n",
      "Epoch 500, Train loss: 1.911\n",
      "Epoch 750, Train loss: 1.651\n",
      "Epoch 1000, Train loss: 1.354\n",
      "Accuracy: 0.760\n",
      "Data length: 150\n",
      "Epoch 250, Train loss: 1.758\n",
      "Epoch 500, Train loss: 1.202\n",
      "Epoch 750, Train loss: 0.734\n",
      "Epoch 1000, Train loss: 0.432\n",
      "Accuracy: 0.980\n"
     ]
    }
   ],
   "source": [
    "LSTM = NeuralNetwork(torch.nn.LSTM)\n",
    "optimizer = torch.optim.SGD(LSTM.parameters(), lr=.001)\n",
    "loss_LSTM = []\n",
    "accuracy_LSTM = []\n",
    "test_model(LSTM, optimizer, loss_LSTM, accuracy_LSTM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.3 GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data length: 25\n",
      "Epoch 250, Train loss: 2.048\n",
      "Epoch 500, Train loss: 1.825\n",
      "Epoch 750, Train loss: 1.623\n",
      "Epoch 1000, Train loss: 1.423\n",
      "Accuracy: 0.600\n",
      "Data length: 75\n",
      "Epoch 250, Train loss: 1.725\n",
      "Epoch 500, Train loss: 1.172\n",
      "Epoch 750, Train loss: 0.739\n",
      "Epoch 1000, Train loss: 0.468\n",
      "Accuracy: 0.987\n",
      "Data length: 150\n",
      "Epoch 250, Train loss: 1.227\n",
      "Epoch 500, Train loss: 0.568\n",
      "Epoch 750, Train loss: 0.281\n",
      "Epoch 1000, Train loss: 0.180\n",
      "Accuracy: 0.993\n"
     ]
    }
   ],
   "source": [
    "GRU = NeuralNetwork(torch.nn.GRU)\n",
    "optimizer = torch.optim.SGD(GRU.parameters(), lr=.001)\n",
    "loss_GRU = []\n",
    "accuracy_GRU = []\n",
    "test_model(GRU, optimizer, loss_GRU, accuracy_GRU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Результаты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RNN accuracy</th>\n",
       "      <th>RNN loss</th>\n",
       "      <th>LSTM accuracy</th>\n",
       "      <th>LSTM loss</th>\n",
       "      <th>GRU accuracy</th>\n",
       "      <th>GRU Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.920000</td>\n",
       "      <td>0.651972</td>\n",
       "      <td>0.48</td>\n",
       "      <td>1.729612</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>1.422651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.148854</td>\n",
       "      <td>0.76</td>\n",
       "      <td>1.353828</td>\n",
       "      <td>0.986667</td>\n",
       "      <td>0.468191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>0.993333</td>\n",
       "      <td>0.081718</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.432106</td>\n",
       "      <td>0.993333</td>\n",
       "      <td>0.179594</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     RNN accuracy  RNN loss  LSTM accuracy  LSTM loss  GRU accuracy  GRU Loss\n",
       "25       0.920000  0.651972           0.48   1.729612      0.600000  1.422651\n",
       "75       1.000000  0.148854           0.76   1.353828      0.986667  0.468191\n",
       "150      0.993333  0.081718           0.98   0.432106      0.993333  0.179594"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = {\n",
    "    'RNN accuracy': accuracy_RNN,\n",
    "    'RNN loss': loss_RNN,\n",
    "    'LSTM accuracy': accuracy_LSTM,\n",
    "    'LSTM loss': loss_LSTM,\n",
    "    'GRU accuracy': accuracy_GRU,\n",
    "    'GRU Loss' : loss_GRU\n",
    "    }\n",
    "\n",
    "# Преобразуйте словарь в DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "df.index = ['25', '75', '150']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучал на 1000 эпох, чтобы достичь максимальных результатов. Как видно в ходе обучения точность и лосс улучшаются по мере роста эпох.\n",
    "Сравнивая 3 модели по соотношению значения точности и лосса, лучший результат показывает RNN, на втором месте GRU, которая показывате неплохие результаты на длинной послеовательности, однако лосс у неё хуже. Самое низкое качество показывает LSTM, предполагаю, что это связано с простотой модели и данных."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
