{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание\n",
    "Задание 1.\n",
    "Обучите нейронную сеть решать шифр Цезаря.\n",
    "\n",
    "Что необходимо сделать:\n",
    "\n",
    "    Написать алгоритм шифра Цезаря для генерации выборки (сдвиг на К каждой буквы. Например, при сдвиге на 2 буква “А” переходит в букву “В” и тп)\n",
    "    Сделать нейронную сеть\n",
    "    Обучить ее (вход - зашифрованная фраза, выход - дешифрованная фраза)\n",
    "    Проверить качество\n",
    "\n",
    "Задание 2.\n",
    "Выполнить практическую работу из лекционного ноутбука.\n",
    "\n",
    "    Построить RNN-ячейку на основе полносвязных слоев\n",
    "    Применить построенную ячейку для генерации текста с выражениями героев сериала “Симпсоны”\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 1. Шифр Цезаря."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Подготовка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# импорт библиотек\n",
    "import torch\n",
    "import random\n",
    "import time\n",
    "import datetime\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Словарь символов\n",
    "##### Работать буедт с русским языком, цифрами и знаками препинания."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "letters = \"абвгдеёжзийклмнопрстуфхцчшщъыьэюя 1234567890.,!:;\\'\\\"?()\" # полный список допустимых символов\n",
    "letters_len = 33 # количество букв в русском языке\n",
    "full_len = len(letters) # общее количество символов в списке"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. Вспомогательные функции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция сдвига букв для шифра. сдвигает только буквы, знаки препинания не трогает.\n",
    "def encode(text, k = 5):\n",
    "    return [letters[(letters.index(c) + k) % letters_len] if letters.index(c) < letters_len else c for c in text ]\n",
    "def decode(text, k = 5):\n",
    "    return [letters[(letters.index(c) - k) % letters_len] if letters.index(c) < letters_len else c for c in text ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "закодировано:\t['е', 'д', '.']\n",
      "декодировано:\t['а', 'я', '.']\n"
     ]
    }
   ],
   "source": [
    "# проверка\n",
    "enc = encode('ая.')\n",
    "denc = decode(enc)\n",
    "print(f\"закодировано:\\t{enc}\\nдекодировано:\\t{denc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция перевода символов в индексы и обратно\n",
    "def text_to_idx(text):\n",
    "    indices = torch.zeros(len(text))\n",
    "    for i in range(len(text)):\n",
    "        indices[i] = letters.index(text[i])\n",
    "    return indices.int()\n",
    "\n",
    "def idx_to_text(indices):\n",
    "    text = \"\"\n",
    "    for i in indices:\n",
    "        text += letters[int(i)]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0, 32, 44], dtype=torch.int32)\n",
      "ая.\n"
     ]
    }
   ],
   "source": [
    "# проверка\n",
    "print(text_to_idx('ая.'))\n",
    "print(idx_to_text(text_to_idx('ая.')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция кодировки и раскодировки текста в тензор со сдвигом\n",
    "def encode_idx(idx_tens, k = 5):\n",
    "    result = idx_tens.clone().detach()\n",
    "    mask1 = result < letters_len\n",
    "    result[mask1] += k\n",
    "    mask2 = mask1 & (result > letters_len - 1)\n",
    "    result[mask2] -= letters_len\n",
    "    return result\n",
    "\n",
    "def decode_idx(idx_tens, k = 5):\n",
    "    result = idx_tens.clone().detach()\n",
    "    mask1 = result < letters_len\n",
    "    result[mask1] -= k\n",
    "    mask2 = mask1 & (result < 0)\n",
    "    result[mask2] += letters_len\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 5,  4, 44], dtype=torch.int32)\n",
      "tensor([ 0, 32, 44], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "print(encode_idx(text_to_idx('ая.')))\n",
    "print(decode_idx(encode_idx(text_to_idx('ая.'))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "текст на входе:\t\tпроверочный текст для шифрования 123 !\n",
      "ключ шифрования:\t6\n",
      "tensor([16, 17, 15,  2,  5, 17, 15, 24, 14, 28, 10, 33, 19,  5, 11, 18, 19, 33,\n",
      "         4, 12, 32, 33, 25,  9, 21, 17, 15,  2,  0, 14,  9, 32, 33, 34, 35, 36,\n",
      "        33, 46], dtype=torch.int32)\n",
      "tensor([22, 23, 21,  8, 11, 23, 21, 30, 20,  1, 16, 33, 25, 11, 17, 24, 25, 33,\n",
      "        10, 18,  5, 33, 31, 15, 27, 23, 21,  8,  6, 20, 15,  5, 33, 34, 35, 36,\n",
      "        33, 46], dtype=torch.int32)\n",
      "tensor([16, 17, 15,  2,  5, 17, 15, 24, 14, 28, 10, 33, 19,  5, 11, 18, 19, 33,\n",
      "         4, 12, 32, 33, 25,  9, 21, 17, 15,  2,  0, 14,  9, 32, 33, 34, 35, 36,\n",
      "        33, 46], dtype=torch.int32)\n",
      "текст на выходе:\tпроверочный текст для шифрования 123 !\n"
     ]
    }
   ],
   "source": [
    "# проверка шифрования и дешифровки\n",
    "input_text = ('Проверочный текст для шифрования 123 !').lower()\n",
    "shift = 6\n",
    "encoded_text = text_to_idx(input_text)\n",
    "encoded_tensor = encode_idx(encoded_text, shift)\n",
    "decoded_tensor = decode_idx(encoded_tensor, shift)\n",
    "decoded_text = idx_to_text(decoded_tensor)\n",
    "print(f\"текст на входе:\\t\\t{input_text}\\nключ шифрования:\\t{shift}\\n{encoded_text}\\n{encoded_tensor}\\n{decoded_tensor}\\nтекст на выходе:\\t{decoded_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4. Создание и обучение модели."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Идея - сделать модель почти универсальной, т.е. обучить её на достаточно большом наборе символов с разным случайным сдвигом, чтобы модель могла расшифровывать различные комбинации, зашифрованные шифром Цезаря."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# создаю данные для обучения в виде случайного набора индексов, соответствующих символам в словаре\n",
    "DATA_CHARS = 10000\n",
    "X_train = torch.randint(0, full_len - 1, (DATA_CHARS,))\n",
    "X_test = torch.randint(0, full_len - 1, (int(0.3 * DATA_CHARS),))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = torch.nn.Embedding(full_len, 30)\n",
    "# embedding(X_train[:4]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Embedding(full_len, 128),\n",
    "    torch.nn.Linear(128, 64),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(64, full_len)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=.01)\n",
    "BATCH_SIZE = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "range(1, 10)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = range(1,10)\n",
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Time: 0.166, Train loss: 0.212\n",
      "Epoch 1. Time: 0.067, Train loss: 0.000\n",
      "Epoch 2. Time: 0.066, Train loss: 0.000\n",
      "Epoch 3. Time: 0.070, Train loss: 0.000\n",
      "Epoch 4. Time: 0.075, Train loss: 0.000\n",
      "Epoch 5. Time: 0.067, Train loss: 0.000\n",
      "Epoch 6. Time: 0.067, Train loss: 0.000\n",
      "Epoch 7. Time: 0.072, Train loss: 0.000\n",
      "Epoch 8. Time: 0.071, Train loss: 0.000\n",
      "Epoch 9. Time: 0.071, Train loss: 0.000\n"
     ]
    }
   ],
   "source": [
    "k = 5\n",
    "for ep in range(10):\n",
    "    start = time.time()\n",
    "    train_loss = 0.\n",
    "    train_passed = 0\n",
    "    for i in range(int(len(X_train) / BATCH_SIZE)):\n",
    "        batch = X_train[i * BATCH_SIZE:(i + 1) * BATCH_SIZE]\n",
    "        Y = decode_idx(batch, k)\n",
    "        optimizer.zero_grad()\n",
    "        answers = model(batch)\n",
    "        l = loss(answers, Y)\n",
    "        train_loss += l.item()\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "        train_passed += 1\n",
    "    print(\"Epoch {}. Time: {:.3f}, Train loss: {:.3f}\".format(ep, time.time() - start, train_loss / train_passed))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Проверка. Модель показывает 100% точность, но есть одно НО. Данная модель достаточно проставя и неуниверсальная, т.к. она обучалась только на одном ключе шифра Цезаря и, естественно, идеально расшифровывает фразы, зашифрованные с помощью этого ключа.\n",
    "##### Высшим пилотажем бы было создать модель которая могла бы расшифровывать фразы, зашифрованные с помощью любого ключа, однако такую модель, как мне представляется, нужно обучать на очень большом тексте, зашифрованном с помощью разных ключей чего я сейчас не осилю."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test = decode_idx(X_test)\n",
    "accuracy = float((model(X_test).argmax(axis=1) == Y_test).sum() / len(Y_test))\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'проверка 1 точности 2, модели'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrase = \"проверка 1 точности 2, модели\"\n",
    "encoded = encode(phrase, 5)\n",
    "indices = text_to_idx(encoded)\n",
    "decoded = model(indices).argmax(axis = 1)\n",
    "idx_to_text(decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 2. Фразы Симпсонов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Подготовка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# импорт библиотек\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import Counter\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Подготовка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.autograd.set_detect_anomaly(True)\n",
    "batch_size = 100\n",
    "seq_size = 32\n",
    "embedding_size = 64\n",
    "lstm_size = 64\n",
    "gradients_norm = 5\n",
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# мешок слов\n",
    "def doc2words(doc):\n",
    "    words=[]\n",
    "    for line in doc:\n",
    "      words += line.split()\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# удаление пунктуации\n",
    "def removepunct(words):\n",
    "    punct = set(string.punctuation)\n",
    "    words = [''.join([char for char in list(word) if char not in punct]) for word in words]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция генерации батчей\n",
    "def get_batches(words, vocab_to_int, batch_size, seq_size):\n",
    "    # Генерируем батчи для  Xs и Ys: shape = (batchsize * num_batches) * seq_size\n",
    "    word_ints = [vocab_to_int[word] for word in words]\n",
    "    num_batches = int(len(word_ints) / (batch_size * seq_size))\n",
    "    Xs = word_ints[:num_batches*batch_size*seq_size]\n",
    "    Ys = np.zeros_like(Xs)\n",
    "    Ys[:-1] = Xs[1:]\n",
    "    Ys[-1] = Xs[0]\n",
    "    Xs = np.reshape(Xs, (num_batches*batch_size, seq_size))\n",
    "    Ys = np.reshape(Ys, (num_batches*batch_size, seq_size))\n",
    "    \n",
    "    # iterate over rows of Xs and Ys to generate batches\n",
    "    for i in range(0, num_batches*batch_size, batch_size):\n",
    "        yield Xs[i:i+batch_size, :], Ys[i:i+batch_size, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# создание сети\n",
    "class RNNModule(nn.Module):\n",
    "    # initialize RNN module\n",
    "    def __init__(self, n_vocab, seq_size=32, embedding_size=64, lstm_size=64):\n",
    "        super(RNNModule, self).__init__()\n",
    "        self.seq_size = seq_size\n",
    "        self.lstm_size = lstm_size\n",
    "        self.embedding = nn.Embedding(n_vocab, embedding_size)\n",
    "        self.lstm = nn.LSTM(embedding_size,\n",
    "                            lstm_size,\n",
    "                            batch_first=True)\n",
    "        self.dense = nn.Linear(lstm_size, n_vocab)\n",
    "        \n",
    "    def forward(self, x, prev_state):\n",
    "        embed = self.embedding(x)\n",
    "        output, state = self.lstm(embed, prev_state)\n",
    "        logits = self.dense(output)\n",
    "\n",
    "        return logits, state\n",
    "    \n",
    "    def zero_state(self, batch_size):\n",
    "        return (torch.zeros(1, batch_size, self.lstm_size),torch.zeros(1, batch_size, self.lstm_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocab_map(vocab):\n",
    "    # 2 словаря - int to words and word to int\n",
    "    int_to_vocab = {k:w for k,w in enumerate(vocab)}\n",
    "    vocab_to_int = {w:k for k,w in int_to_vocab.items()}\n",
    "    return int_to_vocab, vocab_to_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getvocab(words):\n",
    "  # Словарь из мешка слов\n",
    "    wordfreq = Counter(words)\n",
    "    sorted_wordfreq = sorted(wordfreq, key=wordfreq.get)\n",
    "    return sorted_wordfreq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция потерь и оптимизатор\n",
    "def get_loss_and_train_op(net, lr=0.001):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "    return criterion, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# генерация текста\n",
    "def generate_text(device, net, words, n_vocab, vocab_to_int, int_to_vocab, top_k=5):\n",
    "    net.eval()\n",
    "\n",
    "    state_h, state_c = net.zero_state(1)\n",
    "    state_h = state_h.to(device)\n",
    "    state_c = state_c.to(device)\n",
    "    for w in words:\n",
    "        ix = torch.tensor([[vocab_to_int[w]]]).to(device)\n",
    "        output, (state_h, state_c) = net(ix, (state_h, state_c))\n",
    "    \n",
    "    _, top_ix = torch.topk(output[0], k=top_k)\n",
    "    choices = top_ix.tolist()\n",
    "    choice = np.random.choice(choices[0])\n",
    "\n",
    "    words.append(int_to_vocab[choice])\n",
    "    \n",
    "    for _ in range(100):\n",
    "        ix = torch.tensor([[choice]]).to(device)\n",
    "        output, (state_h, state_c) = net(ix, (state_h, state_c))\n",
    "\n",
    "        _, top_ix = torch.topk(output[0], k=top_k)\n",
    "        choices = top_ix.tolist()\n",
    "        choice = np.random.choice(choices[0])\n",
    "        words.append(int_to_vocab[choice])\n",
    "\n",
    "    print(' '.join(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция обучения сети\n",
    "def train_rnn(words, vocab_to_int, int_to_vocab, n_vocab):\n",
    "    \n",
    "    # ячейка RNN\n",
    "    net = RNNModule(n_vocab, seq_size, embedding_size, lstm_size)\n",
    "    net = net.to(device)\n",
    "    criterion, optimizer = get_loss_and_train_op(net, 0.01)\n",
    "\n",
    "    iteration = 0\n",
    "    \n",
    "    # итерируемся по эпохам\n",
    "    for epoch in tqdm(range(10)): # учиться будет на 10 эпохах\n",
    "        # получаем батчи\n",
    "        batches = get_batches(words, vocab_to_int, batch_size, seq_size)\n",
    "        # инициализируем выход и сккрытое состояние\n",
    "        state_h, state_c = net.zero_state(batch_size)\n",
    "\n",
    "        # Передаем данные на GPU\n",
    "        state_h = state_h.to(device)\n",
    "        state_c = state_c.to(device)\n",
    "        # итерируемся по батчам\n",
    "        for x, y in tqdm(batches):\n",
    "            iteration += 1\n",
    "\n",
    "            # Переходим  в режим обучения\n",
    "            net.train()\n",
    "\n",
    "            # Обнуляем градиенты\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Передаем x и y на GPU\n",
    "            x = torch.tensor(x).to(device)\n",
    "            y = torch.tensor(y).to(device)\n",
    "            \n",
    "            # Модель возвращает логиты, последнее скрытое состояние и новый выход\n",
    "            logits, (state_h, state_c) = net(x, (state_h, state_c))\n",
    "            loss = criterion(logits.transpose(1, 2), y)\n",
    "\n",
    "            state_h = state_h.detach()\n",
    "            state_c = state_c.detach()\n",
    "\n",
    "            loss_value = loss.item()\n",
    "\n",
    "            # back-propagation\n",
    "            loss.backward(retain_graph=True)\n",
    "\n",
    "            _ = torch.nn.utils.clip_grad_norm_(net.parameters(), gradients_norm)\n",
    "            \n",
    "            # Обновляем параметры, выполняя шаг обучения\n",
    "            optimizer.step()\n",
    "\n",
    "            if iteration % 100 == 0:\n",
    "                print('Epoch: {}/{}'.format(epoch, 10),'Iteration: {}'.format(iteration),'Loss: {}'.format(loss_value))\n",
    "\n",
    "            # if iteration % 1000 == 0:\n",
    "                # predict(device, net, flags.initial_words, n_vocab,vocab_to_int, int_to_vocab, top_k=5)\n",
    "                # torch.save(net.state_dict(),'checkpoint_pt/model-{}.pth'.format(iteration))\n",
    "                \n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Загрузка и предобработка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_38914/1556817557.py:1: DtypeWarning: Columns (4,5,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('/home/vk/OneDrive/Образование/Слава/Нетология/16. DLL-30 Deep Learning/Материалы/simpsons_script_lines.csv')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>episode_id</th>\n",
       "      <th>number</th>\n",
       "      <th>raw_text</th>\n",
       "      <th>timestamp_in_ms</th>\n",
       "      <th>speaking_line</th>\n",
       "      <th>character_id</th>\n",
       "      <th>location_id</th>\n",
       "      <th>raw_character_text</th>\n",
       "      <th>raw_location_text</th>\n",
       "      <th>spoken_words</th>\n",
       "      <th>normalized_text</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9549</td>\n",
       "      <td>32</td>\n",
       "      <td>209</td>\n",
       "      <td>Miss Hoover: No, actually, it was a little of ...</td>\n",
       "      <td>848000</td>\n",
       "      <td>True</td>\n",
       "      <td>464.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Miss Hoover</td>\n",
       "      <td>Springfield Elementary School</td>\n",
       "      <td>No, actually, it was a little of both. Sometim...</td>\n",
       "      <td>no actually it was a little of both sometimes ...</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9550</td>\n",
       "      <td>32</td>\n",
       "      <td>210</td>\n",
       "      <td>Lisa Simpson: (NEAR TEARS) Where's Mr. Bergstrom?</td>\n",
       "      <td>856000</td>\n",
       "      <td>True</td>\n",
       "      <td>9.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Lisa Simpson</td>\n",
       "      <td>Springfield Elementary School</td>\n",
       "      <td>Where's Mr. Bergstrom?</td>\n",
       "      <td>wheres mr bergstrom</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9551</td>\n",
       "      <td>32</td>\n",
       "      <td>211</td>\n",
       "      <td>Miss Hoover: I don't know. Although I'd sure l...</td>\n",
       "      <td>856000</td>\n",
       "      <td>True</td>\n",
       "      <td>464.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Miss Hoover</td>\n",
       "      <td>Springfield Elementary School</td>\n",
       "      <td>I don't know. Although I'd sure like to talk t...</td>\n",
       "      <td>i dont know although id sure like to talk to h...</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9552</td>\n",
       "      <td>32</td>\n",
       "      <td>212</td>\n",
       "      <td>Lisa Simpson: That life is worth living.</td>\n",
       "      <td>864000</td>\n",
       "      <td>True</td>\n",
       "      <td>9.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Lisa Simpson</td>\n",
       "      <td>Springfield Elementary School</td>\n",
       "      <td>That life is worth living.</td>\n",
       "      <td>that life is worth living</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9553</td>\n",
       "      <td>32</td>\n",
       "      <td>213</td>\n",
       "      <td>Edna Krabappel-Flanders: The polls will be ope...</td>\n",
       "      <td>864000</td>\n",
       "      <td>True</td>\n",
       "      <td>40.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Edna Krabappel-Flanders</td>\n",
       "      <td>Springfield Elementary School</td>\n",
       "      <td>The polls will be open from now until the end ...</td>\n",
       "      <td>the polls will be open from now until the end ...</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id  episode_id  number  \\\n",
       "0  9549          32     209   \n",
       "1  9550          32     210   \n",
       "2  9551          32     211   \n",
       "3  9552          32     212   \n",
       "4  9553          32     213   \n",
       "\n",
       "                                            raw_text timestamp_in_ms  \\\n",
       "0  Miss Hoover: No, actually, it was a little of ...          848000   \n",
       "1  Lisa Simpson: (NEAR TEARS) Where's Mr. Bergstrom?          856000   \n",
       "2  Miss Hoover: I don't know. Although I'd sure l...          856000   \n",
       "3           Lisa Simpson: That life is worth living.          864000   \n",
       "4  Edna Krabappel-Flanders: The polls will be ope...          864000   \n",
       "\n",
       "  speaking_line character_id  location_id       raw_character_text  \\\n",
       "0          True        464.0          3.0              Miss Hoover   \n",
       "1          True          9.0          3.0             Lisa Simpson   \n",
       "2          True        464.0          3.0              Miss Hoover   \n",
       "3          True          9.0          3.0             Lisa Simpson   \n",
       "4          True         40.0          3.0  Edna Krabappel-Flanders   \n",
       "\n",
       "               raw_location_text  \\\n",
       "0  Springfield Elementary School   \n",
       "1  Springfield Elementary School   \n",
       "2  Springfield Elementary School   \n",
       "3  Springfield Elementary School   \n",
       "4  Springfield Elementary School   \n",
       "\n",
       "                                        spoken_words  \\\n",
       "0  No, actually, it was a little of both. Sometim...   \n",
       "1                             Where's Mr. Bergstrom?   \n",
       "2  I don't know. Although I'd sure like to talk t...   \n",
       "3                         That life is worth living.   \n",
       "4  The polls will be open from now until the end ...   \n",
       "\n",
       "                                     normalized_text word_count  \n",
       "0  no actually it was a little of both sometimes ...         31  \n",
       "1                                wheres mr bergstrom          3  \n",
       "2  i dont know although id sure like to talk to h...         22  \n",
       "3                          that life is worth living          5  \n",
       "4  the polls will be open from now until the end ...         33  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('/home/vk/OneDrive/Образование/Слава/Нетология/16. DLL-30 Deep Learning/Материалы/simpsons_script_lines.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['no actually it was a little of both sometimes when a disease is in all the magazines and all the news shows its only natural that you think you have it',\n",
       " 'wheres mr bergstrom',\n",
       " 'i dont know although id sure like to talk to him he didnt touch my lesson plan what did he teach you',\n",
       " 'that life is worth living',\n",
       " 'the polls will be open from now until the end of recess now just in case any of you have decided to put any thought into this well have our final statements martin',\n",
       " 'i dont think theres anything left to say',\n",
       " 'bart',\n",
       " 'victory party under the slide',\n",
       " 'nan',\n",
       " 'mr bergstrom mr bergstrom']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrases = df['normalized_text'].astype(str).tolist()  # колонка с предобработанными текстами\n",
    "phrases[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# получаем мешок слов, удаляем пунктуацию\n",
    "words = removepunct(doc2words(phrases))\n",
    "# Словарь из мешка слов\n",
    "vocab = getvocab(words)\n",
    "# 2 словаря - int_to_vocab и vocab_to_int\n",
    "int_to_vocab, vocab_to_int = vocab_map(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6118804e09a64156b8bbbe1cb41341b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c52e30b19f6f43f08eb8101b20a7389d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/10 Iteration: 100 Loss: 6.873432636260986\n",
      "Epoch: 0/10 Iteration: 200 Loss: 6.394588470458984\n",
      "Epoch: 0/10 Iteration: 300 Loss: 6.195881366729736\n",
      "Epoch: 0/10 Iteration: 400 Loss: 6.484381198883057\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bbfb239cc9e419b8992499582519a6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10 Iteration: 500 Loss: 6.052926540374756\n",
      "Epoch: 1/10 Iteration: 600 Loss: 5.834512233734131\n",
      "Epoch: 1/10 Iteration: 700 Loss: 5.844120502471924\n",
      "Epoch: 1/10 Iteration: 800 Loss: 5.741043567657471\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ac3f65d95b54895bfa9d03a0fa1619a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/10 Iteration: 900 Loss: 5.632015228271484\n",
      "Epoch: 2/10 Iteration: 1000 Loss: 5.5734639167785645\n",
      "Epoch: 2/10 Iteration: 1100 Loss: 5.623622894287109\n",
      "Epoch: 2/10 Iteration: 1200 Loss: 5.638889789581299\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cb3a53dd08248a981b309cc35e9f195",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3/10 Iteration: 1300 Loss: 5.701300621032715\n",
      "Epoch: 3/10 Iteration: 1400 Loss: 5.505397796630859\n",
      "Epoch: 3/10 Iteration: 1500 Loss: 5.360555648803711\n",
      "Epoch: 3/10 Iteration: 1600 Loss: 5.492495536804199\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b921d73ee384fb5a16565fc7d89bedd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4/10 Iteration: 1700 Loss: 5.550518989562988\n",
      "Epoch: 4/10 Iteration: 1800 Loss: 5.295286655426025\n",
      "Epoch: 4/10 Iteration: 1900 Loss: 5.394376754760742\n",
      "Epoch: 4/10 Iteration: 2000 Loss: 5.351922035217285\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a9b49612a144b35ac262a4597d94335",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5/10 Iteration: 2100 Loss: 5.246644973754883\n",
      "Epoch: 5/10 Iteration: 2200 Loss: 5.304635047912598\n",
      "Epoch: 5/10 Iteration: 2300 Loss: 5.224936485290527\n",
      "Epoch: 5/10 Iteration: 2400 Loss: 5.135441303253174\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2926a7f7ccaa4aa588eeeabc7e4f4413",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6/10 Iteration: 2500 Loss: 5.088631629943848\n",
      "Epoch: 6/10 Iteration: 2600 Loss: 5.22064208984375\n",
      "Epoch: 6/10 Iteration: 2700 Loss: 5.239499092102051\n",
      "Epoch: 6/10 Iteration: 2800 Loss: 5.217926025390625\n",
      "Epoch: 6/10 Iteration: 2900 Loss: 5.269838333129883\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3e205fc253f404fb52e93a0726d8b1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7/10 Iteration: 3000 Loss: 5.032680988311768\n",
      "Epoch: 7/10 Iteration: 3100 Loss: 5.1731791496276855\n",
      "Epoch: 7/10 Iteration: 3200 Loss: 5.002187728881836\n",
      "Epoch: 7/10 Iteration: 3300 Loss: 5.08482551574707\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2097e573d88487997122795434d8cf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8/10 Iteration: 3400 Loss: 5.261453151702881\n",
      "Epoch: 8/10 Iteration: 3500 Loss: 5.096644401550293\n",
      "Epoch: 8/10 Iteration: 3600 Loss: 5.008889675140381\n",
      "Epoch: 8/10 Iteration: 3700 Loss: 4.8966875076293945\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f3ea282957a4ea1a34333ca4a68a1c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9/10 Iteration: 3800 Loss: 4.804150104522705\n",
      "Epoch: 9/10 Iteration: 3900 Loss: 5.081767559051514\n",
      "Epoch: 9/10 Iteration: 4000 Loss: 4.911337375640869\n",
      "Epoch: 9/10 Iteration: 4100 Loss: 4.930315017700195\n"
     ]
    }
   ],
   "source": [
    "rnn_net = train_rnn(words, vocab_to_int, int_to_vocab, len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Генерация реплик"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hey you can see what i said you can see that the big leagues oh my dear boy i dont think im not gonna be on your hands are up in a few days ago you can watch out your heart nan hey hey homer i dont care what you want that the big step to me but im not talking that was a good one night nan oh no i was wondering what i can have a big disappointment and now i love the truth is a very bad idea to me and a lot like a good day i think im\n"
     ]
    }
   ],
   "source": [
    "generate_text(device, rnn_net, ['hey', 'you'], len(vocab), vocab_to_int, int_to_vocab)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
